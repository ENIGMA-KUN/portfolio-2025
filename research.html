<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Projects | Shubham Chakraborty</title>
    <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script src="js/research.js"></script>
    <style>
        .gradient-text {
            background-image: linear-gradient(45deg, #4F46E5, #EC4899);
            -webkit-background-clip: text;
            background-clip: text;
            color: transparent;
        }
        .project-card:hover {
            transform: translateY(-5px);
        }
        .tab-active {
            border-bottom: 3px solid #4F46E5;
            color: white;
        }
    </style>
</head>
<body class="bg-gray-900 text-white">
    <!-- Navbar -->
    <nav class="fixed top-0 w-full bg-gray-900 bg-opacity-90 backdrop-filter backdrop-blur-lg z-50 border-b border-gray-800">
        <div class="container mx-auto px-6 py-4 flex justify-between items-center">
            <a href="index.html" class="text-2xl font-bold">Shubham Chakraborty</a>
            <div class="hidden md:flex space-x-10">
                <a href="research.html" class="text-blue-400 transition-colors">Research</a>
                <a href="publications.html" class="hover:text-blue-400 transition-colors">Publications</a>
                <a href="experience.html" class="hover:text-blue-400 transition-colors">Experience</a>
                <a href="contact.html" class="hover:text-blue-400 transition-colors">Contact</a>
            </div>
            <div class="flex space-x-4">
                <a href="https://github.com/enigma-kun" target="_blank" class="text-gray-400 hover:text-white">
                    <i class="fab fa-github text-xl"></i>
                </a>
                <a href="https://linkedin.com/in/shubham-chakraborty" target="_blank" class="text-gray-400 hover:text-white">
                    <i class="fab fa-linkedin text-xl"></i>
                </a>
                <a href="https://scholar.google.com" target="_blank" class="text-gray-400 hover:text-white">
                    <i class="fas fa-graduation-cap text-xl"></i>
                </a>
            </div>
        </div>
    </nav>

    <!-- Header Section -->
    <section class="pt-32 pb-10 px-6 bg-gradient-to-b from-gray-900 to-gray-800">
        <div class="container mx-auto">
            <h1 class="text-4xl md:text-5xl font-bold mb-6 text-center">
                Research <span class="gradient-text">Projects</span>
            </h1>
            <p class="text-xl text-gray-300 max-w-3xl mx-auto text-center mb-12">
                Exploring cutting-edge research in LLM capability analysis, multimodal systems, medical imaging analysis, and retrieval-augmented generation.
            </p>
            
            <!-- Project Tabs -->
            <div class="flex flex-wrap justify-center mb-10">
                <button class="tab-button tab-active px-6 py-3 mx-2 mb-4 text-lg focus:outline-none" data-tab="llm">
                    LLM Capability Analysis
                </button>
                <button class="tab-button px-6 py-3 mx-2 mb-4 text-lg text-gray-400 focus:outline-none" data-tab="multimodal">
                    Multimodal Systems
                </button>
                <button class="tab-button px-6 py-3 mx-2 mb-4 text-lg text-gray-400 focus:outline-none" data-tab="medical">
                    Medical Imaging
                </button>
                <button class="tab-button px-6 py-3 mx-2 mb-4 text-lg text-gray-400 focus:outline-none" data-tab="rag">
                    RAG Systems
                </button>
            </div>
        </div>
    </section>

    <!-- LLM Capability Analysis Section -->
    <section id="llm-tab" class="tab-content py-20 px-6 bg-gray-900">
        <div class="container mx-auto">
            <div class="grid grid-cols-1 lg:grid-cols-2 gap-16">
                <div>
                    <h2 class="text-3xl font-bold mb-6">
                        <span class="gradient-text">LLM Capability Analysis</span>: E-manifold Framework
                    </h2>
                    <p class="text-gray-300 mb-6">
                        Developed a first-of-its-kind framework for quantifying emergent capability thresholds across LLM architectures (7B-72B), 
                        achieving 87% prediction accuracy using novel E-manifold metrics that uncovered previously unobserved scaling laws for reasoning tasks.
                    </p>
                    
                    <div class="bg-gray-800 rounded-xl p-6 mb-8 border border-gray-700">
                        <h3 class="text-xl font-bold mb-4">Research Highlights</h3>
                        <div class="flex items-center mb-4">
                            <div class="w-12 h-12 bg-blue-600 rounded-full flex items-center justify-center mr-4">
                                <i class="fas fa-brain text-2xl"></i>
                            </div>
                            <div>
                                <h4 class="font-bold">E-manifold Metrics</h4>
                                <p class="text-gray-400">Novel mathematical approach to quantify emergent capabilities</p>
                            </div>
                        </div>
                        
                        <div class="flex items-center mb-4">
                            <div class="w-12 h-12 bg-green-600 rounded-full flex items-center justify-center mr-4">
                                <i class="fas fa-chart-line text-2xl"></i>
                            </div>
                            <div>
                                <h4 class="font-bold">87% Prediction Accuracy</h4>
                                <p class="text-gray-400">35% improvement over traditional evaluation approaches</p>
                            </div>
                        </div>
                        
                        <div class="flex items-center">
                            <div class="w-12 h-12 bg-purple-600 rounded-full flex items-center justify-center mr-4">
                                <i class="fas fa-bolt text-2xl"></i>
                            </div>
                            <div>
                                <h4 class="font-bold">42% Compute Reduction</h4>
                                <p class="text-gray-400">Significantly more efficient than standard methods</p>
                            </div>
                        </div>
                    </div>
                    
                    <div class="flex space-x-4">
                        <a href="https://github.com/enigma-kun/llm-capability-analysis" target="_blank" class="px-6 py-3 bg-blue-600 text-white font-medium rounded-lg hover:bg-blue-700 transition-colors flex items-center">
                            <i class="fab fa-github mr-2"></i> GitHub Repository
                        </a>
                        <a href="#" class="px-6 py-3 bg-gray-800 text-white font-medium rounded-lg hover:bg-gray-700 transition-colors flex items-center">
                            <i class="fas fa-file-alt mr-2"></i> Research Paper
                        </a>
                    </div>
                </div>
                
                <div>
                    <!-- LLM Capability Visualization -->
                    <div class="bg-gray-800 rounded-xl p-6 mb-8 border border-gray-700">
                        <h3 class="text-xl font-bold mb-4">LLM Capability Threshold Analysis</h3>
                        <div id="capability-visualization" class="w-full h-80 bg-gray-900 rounded-lg">
                            <!-- D3.js visualization will go here -->
                        </div>
                    </div>
                    
                    <!-- Metric Comparison Visualization -->
                    <div class="bg-gray-800 rounded-xl p-6 border border-gray-700">
                        <h3 class="text-xl font-bold mb-4">E-manifold vs. Traditional Metrics</h3>
                        <div id="metric-comparison" class="w-full h-80 bg-gray-900 rounded-lg">
                            <!-- D3.js visualization will go here -->
                        </div>
                    </div>
                </div>
            </div>
            
            <!-- Technical Details -->
            <div class="mt-16 bg-gray-800 rounded-xl p-8 border border-gray-700">
                <h3 class="text-2xl font-bold mb-6">Technical Implementation</h3>
                
                <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                    <div>
                        <h4 class="text-xl font-bold mb-4">Problem Statement</h4>
                        <p class="text-gray-300 mb-4">
                            As LLMs scale in size, they demonstrate emergent capabilities that are not present in smaller models. 
                            However, predicting exactly when and how these capabilities will emerge has been challenging, 
                            leading to inefficient allocation of computational resources during model development and evaluation.
                        </p>
                        
                        <div class="bg-gray-900 rounded-lg p-4 mb-4">
                            <h5 class="font-bold mb-2">Key Challenges:</h5>
                            <ul class="list-disc pl-5 text-gray-300 space-y-1">
                                <li>Unreliable prediction of capability thresholds across model scales</li>
                                <li>High computational costs of traditional capability evaluation methods</li>
                                <li>Lack of unified theoretical framework for emergent properties in LLMs</li>
                            </ul>
                        </div>
                        
                        <h4 class="text-xl font-bold mb-4">Methodology</h4>
                        <p class="text-gray-300">
                            We developed the E-manifold metrics framework that maps the latent representation space of language models 
                            to capability indicators. This approach allows us to predict capability emergence with high accuracy 
                            while significantly reducing computational requirements.
                        </p>
                    </div>
                    
                    <div>
                        <h4 class="text-xl font-bold mb-4">Implementation Details</h4>
                        <div class="bg-gray-900 rounded-lg p-4 overflow-auto text-sm">
                            <pre class="text-gray-300">
<code>def compute_e_manifold_metric(model, test_set, capability_dimension):
    """
    Calculates the E-manifold metric for a specific capability dimension.
    
    Args:
        model: The language model to evaluate
        test_set: Evaluation dataset
        capability_dimension: The specific capability to measure
        
    Returns:
        e_score: Scalar value representing capability strength
        confidence: Confidence interval for the measurement
    """
    # Extract model representations
    representations = []
    for sample in test_set:
        output = model.generate_with_hidden_states(sample.input)
        representations.append(output.hidden_states[-1])
    
    # Calculate manifold properties
    manifold = compute_manifold_from_representations(representations)
    
    # Project onto capability dimension
    projection = project_manifold_to_capability(manifold, capability_dimension)
    
    # Calculate E-score and confidence
    e_score = measure_manifold_complexity(projection)
    confidence = bootstrap_confidence_interval(e_score, manifold, n_samples=1000)
    
    return e_score, confidence</code>
                            </pre>
                        </div>
                        
                        <h4 class="text-xl font-bold mt-6 mb-4">Key Results</h4>
                        <table class="w-full text-sm">
                            <thead>
                                <tr class="bg-gray-700">
                                    <th class="p-2 text-left">Capability</th>
                                    <th class="p-2 text-left">Scaling Exponent</th>
                                    <th class="p-2 text-left">Threshold Range</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr class="border-b border-gray-700">
                                    <td class="p-2">Multi-step reasoning</td>
                                    <td class="p-2">0.83</td>
                                    <td class="p-2">10B-14B parameters</td>
                                </tr>
                                <tr class="border-b border-gray-700">
                                    <td class="p-2">Analogical reasoning</td>
                                    <td class="p-2">0.67</td>
                                    <td class="p-2">8B-12B parameters</td>
                                </tr>
                                <tr>
                                    <td class="p-2">Mathematical reasoning</td>
                                    <td class="p-2">1.21</td>
                                    <td class="p-2">15B-22B parameters</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>
            </div>
        </div>
    </section>
    
    <!-- Multimodal Systems Section -->
    <section id="multimodal-tab" class="tab-content py-20 px-6 bg-gray-900 hidden">
        <div class="container mx-auto">
            <div class="grid grid-cols-1 lg:grid-cols-2 gap-16">
                <div>
                    <h2 class="text-3xl font-bold mb-6">
                        <span class="gradient-text">Multimodal Learning</span>: Cross-Modal Fusion System
                    </h2>
                    <p class="text-gray-300 mb-6">
                        Architected cross-modal fusion system integrating Vision Transformer (ViT) image encodings with BERT text embeddings,
                        achieving 85% accuracy on zero-shot multimodal reasoning tasks with a custom attention mechanism for contextual alignment.
                    </p>
                    
                    <div class="bg-gray-800 rounded-xl p-6 mb-8 border border-gray-700">
                        <h3 class="text-xl font-bold mb-4">Research Highlights</h3>
                        <div class="flex items-center mb-4">
                            <div class="w-12 h-12 bg-teal-600 rounded-full flex items-center justify-center mr-4">
                                <i class="fas fa-image text-2xl"></i>
                            </div>
                            <div>
                                <h4 class="font-bold">Vision-Language Integration</h4>
                                <p class="text-gray-400">Cross-modal fusion with ViT and BERT encodings</p>
                            </div>
                        </div>
                        
                        <div class="flex items-center mb-4">
                            <div class="w-12 h-12 bg-blue-600 rounded-full flex items-center justify-center mr-4">
                                <i class="fas fa-bullseye text-2xl"></i>
                            </div>
                            <div>
                                <h4 class="font-bold">85% Zero-shot Accuracy</h4>
                                <p class="text-gray-400">Impressive performance on unseen multimodal tasks</p>
                            </div>
                        </div>
                        
                        <div class="flex items-center">
                            <div class="w-12 h-12 bg-purple-600 rounded-full flex items-center justify-center mr-4">
                                <i class="fas fa-network-wired text-2xl"></i>
                            </div>
                            <div>
                                <h4 class="font-bold">Custom Attention Mechanism</h4>
                                <p class="text-gray-400">Novel approach for cross-modal contextual alignment</p>
                            </div>
                        </div>
                    </div>
                    
                    <div class="flex space-x-4">
                        <a href="https://github.com/enigma-kun/multimodal-fusion" target="_blank" class="px-6 py-3 bg-teal-600 text-white font-medium rounded-lg hover:bg-teal-700 transition-colors flex items-center">
                            <i class="fab fa-github mr-2"></i> GitHub Repository
                        </a>
                        <a href="#" class="px-6 py-3 bg-gray-800 text-white font-medium rounded-lg hover:bg-gray-700 transition-colors flex items-center">
                            <i class="fas fa-file-alt mr-2"></i> Research Paper
                        </a>
                    </div>
                </div>
                
                <div>
                    <!-- Multimodal Architecture Visualization -->
                    <div class="bg-gray-800 rounded-xl p-6 border border-gray-700">
                        <h3 class="text-xl font-bold mb-4">Multimodal Fusion Architecture</h3>
                        <div id="multimodal-architecture" class="w-full h-96 bg-gray-900 rounded-lg">
                            <!-- D3.js visualization will go here -->
                        </div>
                    </div>
                </div>
            </div>
            
            <!-- Technical Details -->
            <div class="mt-16 bg-gray-800 rounded-xl p-8 border border-gray-700">
                <h3 class="text-2xl font-bold mb-6">Technical Implementation</h3>
                
                <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                    <div>
                        <h4 class="text-xl font-bold mb-4">Problem Statement</h4>
                        <p class="text-gray-300 mb-4">
                            Traditional multimodal systems struggle with contextual alignment between visual and textual data, 
                            especially in zero-shot scenarios with limited training examples. Our research addresses the challenge 
                            of creating a unified representation space for both modalities.
                        </p>
                        
                        <div class="bg-gray-900 rounded-lg p-4 mb-4">
                            <h5 class="font-bold mb-2">Key Challenges:</h5>
                            <ul class="list-disc pl-5 text-gray-300 space-y-1">
                                <li>Aligning representations from disparate modalities</li>
                                <li>Maintaining contextual coherence across modalities</li>
                                <li>Enabling zero-shot transfer to unseen tasks</li>
                                <li>Handling varying degrees of information density in each modality</li>
                            </ul>
                        </div>
                        
                        <h4 class="text-xl font-bold mb-4">Methodology</h4>
                        <p class="text-gray-300">
                            We designed a cross-modal fusion system that integrates ViT image encodings with BERT text embeddings 
                            through a custom attention mechanism. This approach allows for flexible information exchange between 
                            modalities and enables robust performance on zero-shot reasoning tasks.
                        </p>
                    </div>
                    
                    <div>
                        <h4 class="text-xl font-bold mb-4">Implementation Details</h4>
                        <div class="bg-gray-900 rounded-lg p-4 overflow-auto text-sm">
                            <pre class="text-gray-300">
<code>class CrossModalFusion(nn.Module):
    def __init__(self, vision_dim=768, language_dim=768, fusion_dim=512):
        super().__init__()
        self.vision_projection = nn.Linear(vision_dim, fusion_dim)
        self.language_projection = nn.Linear(language_dim, fusion_dim)
        
        # Multi-head cross-attention
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=fusion_dim,
            num_heads=8,
            dropout=0.1
        )
        
        # Final fusion layer
        self.fusion_layer = nn.Sequential(
            nn.Linear(fusion_dim * 2, fusion_dim),
            nn.LayerNorm(fusion_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
    
    def forward(self, vision_features, language_features):
        # Project to common dimension
        v_proj = self.vision_projection(vision_features)
        l_proj = self.language_projection(language_features)
        
        # Cross-attention: vision attending to language
        v_attended, _ = self.cross_attention(
            query=v_proj,
            key=l_proj,
            value=l_proj
        )
        
        # Cross-attention: language attending to vision
        l_attended, _ = self.cross_attention(
            query=l_proj,
            key=v_proj,
            value=v_proj
        )
        
        # Concatenate and fuse
        fused = torch.cat([v_attended, l_attended], dim=-1)
        output = self.fusion_layer(fused)
        
        return output</code>
                            </pre>
                        </div>
                        
                        <h4 class="text-xl font-bold mt-6 mb-4">Performance on Multimodal Tasks</h4>
                        <table class="w-full text-sm">
                            <thead>
                                <tr class="bg-gray-700">
                                    <th class="p-2 text-left">Task</th>
                                    <th class="p-2 text-center">Our Model</th>
                                    <th class="p-2 text-center">SOTA</th>
                                    <th class="p-2 text-center">Improvement</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr class="border-b border-gray-700">
                                    <td class="p-2">Visual Question Answering</td>
                                    <td class="p-2 text-center">85.2%</td>
                                    <td class="p-2 text-center">82.1%</td>
                                    <td class="p-2 text-center text-green-500">+3.1%</td>
                                </tr>
                                <tr class="border-b border-gray-700">
                                    <td class="p-2">Image-Text Retrieval</td>
                                    <td class="p-2 text-center">79.8%</td>
                                    <td class="p-2 text-center">76.4%</td>
                                    <td class="p-2 text-center text-green-500">+3.4%</td>
                                </tr>
                                <tr>
                                    <td class="p-2">Visual Reasoning</td>
                                    <td class="p-2 text-center">71.5%</td>
                                    <td class="p-2 text-center">65.9%</td>
                                    <td class="p-2 text-center text-green-500">+5.6%</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>
            </div>
        </div>
    </section>
    
    <!-- Medical Imaging Section -->
    <section id="medical-tab" class="tab-content py-20 px-6 bg-gray-900 hidden">
        <div class="container mx-auto">
            <div class="grid grid-cols-1 lg:grid-cols-2 gap-16">
                <div>
                    <h2 class="text-3xl font-bold mb-6">
                        <span class="gradient-text">Medical Image Analysis</span>: Enhanced Segmentation
                    </h2>
                    <p class="text-gray-300 mb-6">
                        Processed over 3,000 medical images with U-Net architecture and transformer enhancements, 
                        achieving top 15% ranking in ISBI Challenge through advanced segmentation approach maintaining anatomical consistency.
                    </p>
                    
                    <div class="bg-gray-800 rounded-xl p-6 mb-8 border border-gray-700">
                        <h3 class="text-xl font-bold mb-4">Research Highlights</h3>
                        <div class="flex items-center mb-4">
                            <div class="w-12 h-12 bg-red-600 rounded-full flex items-center justify-center mr-4">
                                <i class="fas fa-heartbeat text-2xl"></i>
                            </div>
                            <div>
                                <h4 class="font-bold">U-Net + Transformer Architecture</h4>
                                <p class="text-gray-400">Enhanced segmentation with global context</p>
                            </div>
                        </div>
                        
                        <div class="flex items-center mb-4">
                            <div class="w-12 h-12 bg-blue-600 rounded-full flex items-center justify-center mr-4">
                                <i class="fas fa-check-circle text-2xl"></i>
                            </div>
                            <div>
                                <h4 class="font-bold">92% Segmentation Accuracy</h4>
                                <p class="text-gray-400">Top 15% ranking in ISBI Challenge</p>
                            </div>
                        </div>
                        
                        <div class="flex items-center">
                            <div class="w-12 h-12 bg-green-600 rounded-full flex items-center justify-center mr-4">
                                <i class="fas fa-filter text-2xl"></i>
                            </div>
                            <div>
                                <h4 class="font-bold">60% Reduction in False Positives</h4>
                                <p class="text-gray-400">While maintaining high sensitivity for critical diagnostics</p>
                            </div>
                        </div>
                    </div>
                    
                    <div class="flex space-x-4">
                        <a href="https://github.com/enigma-kun/medical-segmentation" target="_blank" class="px-6 py-3 bg-red-600 text-white font-medium rounded-lg hover:bg-red-700 transition-colors flex items-center">
                            <i class="fab fa-github mr-2"></i> GitHub Repository
                        </a>
                        <a href="#" class="px-6 py-3 bg-gray-800 text-white font-medium rounded-lg hover:bg-gray-700 transition-colors flex items-center">
                            <i class="fas fa-file-alt mr-2"></i> Research Paper
                        </a>
                    </div>
                </div>
                
                <div>
                    <!-- Medical Image Visualization -->
                    <div class="bg-gray-800 rounded-xl p-6 border border-gray-700">
                        <h3 class="text-xl font-bold mb-4">Segmentation Performance Metrics</h3>
                        <div id="medical-visualization" class="w-full h-96 bg-gray-900 rounded-lg">
                            <!-- D3.js visualization will go here -->
                        </div>
                    </div>
                </div>
            </div>
            
            <!-- Technical Details -->
            <div class="mt-16 bg-gray-800 rounded-xl p-8 border border-gray-700">
                <h3 class="text-2xl font-bold mb-6">Technical Implementation</h3>
                
                <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                    <div>
                        <h4 class="text-xl font-bold mb-4">Problem Statement</h4>
                        <p class="text-gray-300 mb-4">
                            Medical image segmentation requires high precision and anatomical consistency, which traditional approaches often struggle with.
                            Our research focused on maintaining structural coherence while achieving high accuracy in segmenting complex medical structures.
                        </p>
                        
                        <div class="bg-gray-900 rounded-lg p-4 mb-4">
                            <h5 class="font-bold mb-2">Key Challenges:</h5>
                            <ul class="list-disc pl-5 text-gray-300 space-y-1">
                                <li>Maintaining anatomical consistency in segmentation outputs</li>
                                <li>Handling variable tissue appearance and intensity</li>
                                <li>Limited availability of annotated medical data</li>
                                <li>Balancing sensitivity and specificity for clinical applications</li>
                            </ul>
                        </div>
                        
                        <h4 class="text-xl font-bold mb-4">Methodology</h4>
                        <p class="text-gray-300">
                            We enhanced the U-Net architecture with transformer modules for global context understanding,
                            implemented adaptive augmentation strategies, and enforced anatomical consistency through
                            specialized loss functions that penalize anatomically implausible segmentations.
                        </p>
                    </div>
                    
                    <div>
                        <h4 class="text-xl font-bold mb-4">Implementation Details</h4>
                        <div class="bg-gray-900 rounded-lg p-4 overflow-auto text-sm">
                            <pre class="text-gray-300">
<code>class TransformerUNet(nn.Module):
    def __init__(self, in_channels=1, out_channels=1, features=[64, 128, 256, 512]):
        super().__init__()
        self.ups = nn.ModuleList()
        self.downs = nn.ModuleList()
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # Down part of UNet
        for feature in features:
            self.downs.append(DoubleConv(in_channels, feature))
            in_channels = feature
        
        # Transformer block for global context
        self.transformer = TransformerBlock(
            dim=features[-1],
            heads=8,
            dim_head=64,
            mlp_dim=features[-1] * 2
        )
        
        # Up part of UNet
        for feature in reversed(features):
            self.ups.append(
                nn.ConvTranspose2d(
                    feature * 2, feature, kernel_size=2, stride=2
                )
            )
            self.ups.append(DoubleConv(feature * 2, feature))
        
        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)
        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)
        
        # Anatomical consistency module
        self.anatomical_consistency = AnatomicalConsistencyModule()
    
    def forward(self, x):
        skip_connections = []
        
        # Encoder pathway
        for down in self.downs:
            x = down(x)
            skip_connections.append(x)
            x = self.pool(x)
        
        # Transformer for global context
        b, c, h, w = x.shape
        x = x.flatten(2).transpose(1, 2)  # (b, h*w, c)
        x = self.transformer(x)
        x = x.transpose(1, 2).view(b, c, h, w)  # (b, c, h, w)
        
        x = self.bottleneck(x)
        skip_connections = skip_connections[::-1]
        
        # Decoder pathway
        for idx in range(0, len(self.ups), 2):
            x = self.ups[idx](x)
            skip_connection = skip_connections[idx // 2]
            
            if x.shape != skip_connection.shape:
                x = nn.functional.resize(x, size=skip_connection.shape[2:])
                
            concat_skip = torch.cat((skip_connection, x), dim=1)
            x = self.ups[idx + 1](concat_skip)
        
        # Apply anatomical consistency constraints
        x = self.anatomical_consistency(x)
        
        return self.final_conv(x)</code>
                            </pre>
                        </div>
                        
                        <h4 class="text-xl font-bold mt-6 mb-4">Data Augmentation Strategy</h4>
                        <div class="bg-gray-900 rounded-lg p-4 text-gray-300">
                            <ul class="list-disc pl-5 space-y-2">
                                <li><strong>Elastic Deformation:</strong> Random elastic transformations to simulate anatomical variability</li>
                                <li><strong>Intensity Adjustments:</strong> Random brightness, contrast, and gamma adjustments to handle scanner variations</li>
                                <li><strong>Noise Injection:</strong> Random Gaussian and speckle noise to improve robustness</li>
                                <li><strong>Patch-based Training:</strong> Random extraction of patches with class-balanced sampling to handle class imbalance</li>
                                <li><strong>Mixup:</strong> Linear combinations of samples and their labels to regularize the model</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    
    <!-- RAG Systems Section -->
    <section id="rag-tab" class="tab-content py-20 px-6 bg-gray-900 hidden">
        <div class="container mx-auto">
            <div class="grid grid-cols-1 lg:grid-cols-2 gap-16">
                <div>
                    <h2 class="text-3xl font-bold mb-6">
                        <span class="gradient-text">Retrieval-Augmented Generation</span>: Advanced RAG System
                    </h2>
                    <p class="text-gray-300 mb-6">
                        Built a comprehensive RAG system combining vector databases with fine-tuned LLMs for academic information retrieval,
                        achieving 78% improvement in answer accuracy and reducing hallucinations by 65% across multiple domains.
                    </p>
                    
                    <div class="bg-gray-800 rounded-xl p-6 mb-8 border border-gray-700">
                        <h3 class="text-xl font-bold mb-4">Research Highlights</h3>
                        <div class="flex items-center mb-4">
                            <div class="w-12 h-12 bg-yellow-600 rounded-full flex items-center justify-center mr-4">
                                <i class="fas fa-database text-2xl"></i>
                            </div>
                            <div>
                                <h4 class="font-bold">Vector Database Integration</h4>
                                <p class="text-gray-400">Efficient semantic search and retrieval</p>
                            </div>
                        </div>
                        
                        <div class="flex items-center mb-4">
                            <div class="w-12 h-12 bg-blue-600 rounded-full flex items-center justify-center mr-4">
                                <i class="fas fa-check-double text-2xl"></i>
                            </div>
                            <div>
                                <h4 class="font-bold">78% Accuracy Improvement</h4>
                                <p class="text-gray-400">Significant enhancement in answer precision</p>
                            </div>
                        </div>
                        
                        <div class="flex items-center">
                            <div class="w-12 h-12 bg-green-600 rounded-full flex items-center justify-center mr-4">
                                <i class="fas fa-ghost text-2xl"></i>
                            </div>
                            <div>
                                <h4 class="font-bold">65% Reduction in Hallucinations</h4>
                                <p class="text-gray-400">More reliable and factual responses</p>
                            </div>
                        </div>
                    </div>
                    
                    <div class="flex space-x-4">
                        <a href="https://github.com/enigma-kun/advanced-rag" target="_blank" class="px-6 py-3 bg-yellow-600 text-white font-medium rounded-lg hover:bg-yellow-700 transition-colors flex items-center">
                            <i class="fab fa-github mr-2"></i> GitHub Repository
                        </a>
                        <a href="#" class="px-6 py-3 bg-gray-800 text-white font-medium rounded-lg hover:bg-gray-700 transition-colors flex items-center">
                            <i class="fas fa-file-alt mr-2"></i> Research Paper
                        </a>
                    </div>
                </div>
                
                <div>
                    <!-- RAG Visualization -->
                    <div class="bg-gray-800 rounded-xl p-6 border border-gray-700">
                        <h3 class="text-xl font-bold mb-4">RAG System Performance</h3>
                        <div id="rag-visualization" class="w-full h-96 bg-gray-900 rounded-lg">
                            <!-- D3.js visualization will go here -->
                        </div>
                    </div>
                </div>
            </div>
            
            <!-- Technical Details -->
            <div class="mt-16 bg-gray-800 rounded-xl p-8 border border-gray-700">
                <h3 class="text-2xl font-bold mb-6">Technical Implementation</h3>
                
                <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                    <div>
                        <h4 class="text-xl font-bold mb-4">Problem Statement</h4>
                        <p class="text-gray-300 mb-4">
                            Standard LLM implementations struggle with hallucinations and factual accuracy when answering domain-specific 
                            academic questions. Our research focused on enhancing the reliability and accuracy of LLM responses by integrating
                            retrieval mechanisms with generation capabilities.
                        </p>
                        
                        <div class="bg-gray-900 rounded-lg p-4 mb-4">
                            <h5 class="font-bold mb-2">Key Challenges:</h5>
                            <ul class="list-disc pl-5 text-gray-300 space-y-1">
                                <li>Reducing hallucinations in LLM responses</li>
                                <li>Improving factual accuracy for domain-specific queries</li>
                                <li>Efficiently retrieving relevant context for generation</li>
                                <li>Balancing retrieval and generation components</li>
                            </ul>
                        </div>
                        
                        <h4 class="text-xl font-bold mb-4">Methodology</h4>
                        <p class="text-gray-300">
                            We built a comprehensive RAG system that combines vector databases for efficient semantic retrieval
                            with fine-tuned LLMs for context-aware generation. The system incorporates advanced prompt engineering
                            techniques and optimized context windows to improve the overall quality of responses.
                        </p>
                    </div>
                    
                    <div>
                        <h4 class="text-xl font-bold mb-4">System Architecture</h4>
                        <div class="bg-gray-900 rounded-lg p-4 overflow-auto text-sm">
                            <pre class="text-gray-300">
<code>class AdvancedRAGSystem:
    def __init__(self, 
                 retriever_model="text-embedding-ada-002",
                 generator_model="gpt-3.5-turbo",
                 top_k=5,
                 similarity_threshold=0.75):
        self.embedder = OpenAIEmbeddings(model=retriever_model)
        self.vector_store = FAISS(embedding_function=self.embedder)
        self.generator = ChatOpenAI(model_name=generator_model)
        self.top_k = top_k
        self.similarity_threshold = similarity_threshold
        
        # Advanced prompt template with context integration
        self.prompt_template = """
        Answer the question based on the following context:
        
        {context}
        
        Question: {question}
        
        Important guidelines:
        1. Answer only based on the provided context
        2. If the context doesn't contain the answer, say "I don't have enough information"
        3. Cite specific sources from the context when possible
        4. Do not include any information not present in the context
        
        Answer:
        """
        
    def add_documents(self, documents):
        """Add documents to the vector store."""
        self.vector_store.add_documents(documents)
        
    def retrieve(self, query):
        """Retrieve relevant documents for the query."""
        # Calculate embeddings for the query
        query_embedding = self.embedder.embed_query(query)
        
        # Retrieve top_k most similar documents
        docs_with_scores = self.vector_store.similarity_search_with_score(
            query, self.top_k
        )
        
        # Filter by similarity threshold
        relevant_docs = [
            doc for doc, score in docs_with_scores 
            if score >= self.similarity_threshold
        ]
        
        return relevant_docs
        
    def generate(self, query, retrieved_docs):
        """Generate response based on retrieved documents."""
        # Prepare context from retrieved documents
        context = "\n\n".join([doc.page_content for doc in retrieved_docs])
        
        # Format prompt with context and query
        prompt = self.prompt_template.format(
            context=context,
            question=query
        )
        
        # Generate response using LLM
        response = self.generator.generate(prompt)
        
        return response
        
    def query(self, query):
        """End-to-end RAG pipeline."""
        # Retrieve relevant documents
        retrieved_docs = self.retrieve(query)
        
        # Generate response
        if retrieved_docs:
            response = self.generate(query, retrieved_docs)
            return {
                "response": response,
                "sources": [doc.metadata for doc in retrieved_docs],
                "confidence": len(retrieved_docs) / self.top_k
            }
        else:
            return {
                "response": "I don't have enough information to answer this question.",
                "sources": [],
                "confidence": 0.0
            }</code>
                            </pre>
                        </div>
                        
                        <h4 class="text-xl font-bold mt-6 mb-4">Performance Across Domains</h4>
                        <table class="w-full text-sm">
                            <thead>
                                <tr class="bg-gray-700">
                                    <th class="p-2 text-left">Domain</th>
                                    <th class="p-2 text-center">Baseline</th>
                                    <th class="p-2 text-center">RAG System</th>
                                    <th class="p-2 text-center">Improvement</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr class="border-b border-gray-700">
                                    <td class="p-2">Medicine</td>
                                    <td class="p-2 text-center">52%</td>
                                    <td class="p-2 text-center">91%</td>
                                    <td class="p-2 text-center text-green-500">+39%</td>
                                </tr>
                                <tr class="border-b border-gray-700">
                                    <td class="p-2">Computer Science</td>
                                    <td class="p-2 text-center">58%</td>
                                    <td class="p-2 text-center">89%</td>
                                    <td class="p-2 text-center text-green-500">+31%</td>
                                </tr>
                                <tr>
                                    <td class="p-2">Law</td>
                                    <td class="p-2 text-center">45%</td>
                                    <td class="p-2 text-center">87%</td>
                                    <td class="p-2 text-center text-green-500">+42%</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>
            </div>
        </div>
    </section>
    
    <!-- Footer -->
    <footer class="bg-gray-950 py-12 border-t border-gray-800">
        <div class="container mx-auto px-6">
            <div class="flex flex-col md:flex-row justify-between items-center">
                <div class="mb-8 md:mb-0">
                    <h3 class="text-2xl font-bold mb-2">Shubham Chakraborty</h3>
                    <p class="text-gray-400 max-w-md">
                        AI/ML Researcher focused on advancing LLM capabilities, multimodal systems, and medical imaging analysis.
                    </p>
                </div>
                <div class="flex space-x-6">
                    <a href="https://github.com/enigma-kun" target="_blank" class="text-gray-400 hover:text-white">
                        <i class="fab fa-github text-2xl"></i>
                    </a>
                    <a href="https://linkedin.com/in/shubham-chakraborty" target="_blank" class="text-gray-400 hover:text-white">
                        <i class="fab fa-linkedin text-2xl"></i>
                    </a>
                    <a href="https://scholar.google.com" target="_blank" class="text-gray-400 hover:text-white">
                        <i class="fas fa-graduation-cap text-2xl"></i>
                    </a>
                    <a href="mailto:chakraborty.shubham007@gmail.com" class="text-gray-400 hover:text-white">
                        <i class="fas fa-envelope text-2xl"></i>
                    </a>
                </div>
            </div>
            <div class="mt-8 pt-8 border-t border-gray-800 flex flex-col md:flex-row justify-between items-center">
                <p class="text-gray-500 text-sm mb-4 md:mb-0">
                    &copy; 2025 Shubham Chakraborty. All rights reserved.
                </p>
                <div class="flex space-x-8">
                    <a href="index.html" class="text-gray-400 hover:text-white text-sm">Home</a>
                    <a href="research.html" class="text-gray-400 hover:text-white text-sm">Research</a>
                    <a href="publications.html" class="text-gray-400 hover:text-white text-sm">Publications</a>
                    <a href="experience.html" class="text-gray-400 hover:text-white text-sm">Experience</a>
                    <a href="contact.html" class="text-gray-400 hover:text-white text-sm">Contact</a>
                </div>
            </div>
        </div>
    </footer>

    <!-- Tab Switching Script -->
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // Initialize visualizations from research.js
            
            // Tab switching functionality
            const tabs = document.querySelectorAll('.tab-button');
            const tabContents = document.querySelectorAll('.tab-content');
            
            tabs.forEach(tab => {
                tab.addEventListener('click', () => {
                    // Remove active class from all tabs
                    tabs.forEach(t => {
                        t.classList.remove('tab-active');
                        t.classList.add('text-gray-400');
                    });
                    
                    // Add active class to clicked tab
                    tab.classList.add('tab-active');
                    tab.classList.remove('text-gray-400');
                    
                    // Hide all tab contents
                    tabContents.forEach(content => {
                        content.classList.add('hidden');
                    });
                    
                    // Show corresponding tab content
                    const tabId = tab.dataset.tab;
                    document.getElementById(`${tabId}-tab`).classList.remove('hidden');
                    
                    // Redraw visualizations for the active tab
                    if (tabId === 'llm') {
                        if (typeof initCapabilityVisualization === 'function') {
                            initCapabilityVisualization();
                        }
                        if (typeof initMetricComparisonVisualization === 'function') {
                            initMetricComparisonVisualization();
                        }
                    } else if (tabId === 'multimodal') {
                        if (typeof initMultimodalArchitecture === 'function') {
                            initMultimodalArchitecture();
                        }
                    } else if (tabId === 'medical') {
                        if (typeof initMedicalImageVisualization === 'function') {
                            initMedicalImageVisualization();
                        }
                    } else if (tabId === 'rag') {
                        if (typeof initRAGVisualization === 'function') {
                            initRAGVisualization();
                        }
                    }
                });
            });
        });
    </script>
</body>
</html>